{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6d8b35",
   "metadata": {},
   "source": [
    "# Recast Whole-Brain MRI to Pituitary MRI\n",
    "\n",
    "- Load Data\n",
    "- Pre-process\n",
    "- Inference\n",
    "- Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e573258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, glob, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "\n",
    "\n",
    "# transforms\n",
    "from fastai.basics import *\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    NormalizeIntensityd,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818edd5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4855bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import VNet, UNet\n",
    "\n",
    "ensemble_models = {\n",
    "    \"UNET3D_dice_loss\": UNet(\n",
    "                    dimensions=3,\n",
    "                    in_channels=1,\n",
    "                    out_channels=2,\n",
    "                    channels=(16, 32, 64, 128, 256),\n",
    "                    strides=(2, 2, 2, 2),\n",
    "                    num_res_units=2,\n",
    "                    dropout=0.0,\n",
    "                ),\n",
    "\n",
    "    \"VNET_dice_loss\": VNet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=2,\n",
    "            ),\n",
    "    \n",
    "    \"CONDSEG_dice_loss\": UNet(\n",
    "                    dimensions=3,\n",
    "                    in_channels=3,\n",
    "                    out_channels=2,\n",
    "                    channels=(16, 32, 64, 128, 256),\n",
    "                    strides=(2, 2, 2, 2),\n",
    "                    num_res_units=2,\n",
    "                    dropout=0.0,\n",
    "                )\n",
    "}\n",
    "\n",
    "# Load pretrained weights\n",
    "for model_name, model_arch in ensemble_models.items():\n",
    "    loc = torch.load(f'ensemble_models/{model_name}/model.pth')\n",
    "    model_arch.load_state_dict(loc['model'])\n",
    "    \n",
    "# Loss\n",
    "def dice(input, target):\n",
    "    iflat = input.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2. * intersection) /\n",
    "           (iflat.sum() + tflat.sum()))\n",
    "\n",
    "def dice_score(input, target):\n",
    "    return dice(input.argmax(1), target)\n",
    "\n",
    "def dice_loss(input, target): \n",
    "    return 1 - dice(input.softmax(1)[:, 1], target)\n",
    "\n",
    "def ce_loss(input, target):\n",
    "    return torch.nn.BCEWithLogitsLoss()(input[:, 1], target.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3880c65",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "- Data source: LONI Imaging & Data Archive (https://ida.loni.usc.edu/login.jsp). \n",
    "- Datasets: ABIDE, ABVIB, ADNI1_Complete_1Yr_1.5T, AIBL, ICMB, and PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2ba84f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>imputedSeq</th>\n",
       "      <th>sz</th>\n",
       "      <th>px</th>\n",
       "      <th>sp</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABIDE/ABIDE_1/50412/MP-RAGE/2000-01-01_00_00_00.0/S164292</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(106, 256, 256)</td>\n",
       "      <td>16-bit signed integer</td>\n",
       "      <td>(1.4, 1.0, 1.0)</td>\n",
       "      <td>(1, 0, 0, 0, -1, 0, 0, 0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABVIB/ABVIB/3830/MPRAGE/2012-06-18_11_46_49.0/S341930</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(256, 256, 192)</td>\n",
       "      <td>16-bit unsigned integer</td>\n",
       "      <td>(1.0, 1.0, 1.0)</td>\n",
       "      <td>(0, 0, -1, 1, 0, 0, 0, -1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADNI/ADNI1_Complete_1Yr_1.5T/ADNI/023_S_0139/MPR-R__GradWarp__B1_Correction__N3__Scaled/2007-02-09_09_44_27.0/S26343</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(192, 192, 160)</td>\n",
       "      <td>32-bit float</td>\n",
       "      <td>(1.26, 1.25, 1.19)</td>\n",
       "      <td>(0, 0, 1, 0, 1, 0, -1, 0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIBL/AIBL/338/MPRAGE_SAG_ISO_p2_ND/2012-10-06_11_32_11.0/S231118</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(256, 256, 176)</td>\n",
       "      <td>16-bit unsigned integer</td>\n",
       "      <td>(1.0, 1.0, 1.0)</td>\n",
       "      <td>(0, 0, -1, 1, 0, 0, 0, -1, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICMB/ICBM/UTHC_1098/MPRAGE_T1_AX_0.8_mm_TI-780/2009-03-13_13_01_09.0/S68959</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(220, 320, 208)</td>\n",
       "      <td>16-bit unsigned integer</td>\n",
       "      <td>(0.8, 0.8, 1.0)</td>\n",
       "      <td>(1, 0, 0, 0, 1, 0, 0, 0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PPMI/PPMI/3505/MPRAGEadni/2010-12-23_10_50_52.0/S189286</td>\n",
       "      <td>MPR</td>\n",
       "      <td>(288, 288, 170)</td>\n",
       "      <td>16-bit unsigned integer</td>\n",
       "      <td>(0.92, 0.92, 1.2)</td>\n",
       "      <td>(0, 0, -1, 1, 0, 0, 0, -1, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     fn  \\\n",
       "0                                                             ABIDE/ABIDE_1/50412/MP-RAGE/2000-01-01_00_00_00.0/S164292   \n",
       "1                                                                 ABVIB/ABVIB/3830/MPRAGE/2012-06-18_11_46_49.0/S341930   \n",
       "2  ADNI/ADNI1_Complete_1Yr_1.5T/ADNI/023_S_0139/MPR-R__GradWarp__B1_Correction__N3__Scaled/2007-02-09_09_44_27.0/S26343   \n",
       "3                                                      AIBL/AIBL/338/MPRAGE_SAG_ISO_p2_ND/2012-10-06_11_32_11.0/S231118   \n",
       "4                                           ICMB/ICBM/UTHC_1098/MPRAGE_T1_AX_0.8_mm_TI-780/2009-03-13_13_01_09.0/S68959   \n",
       "5                                                               PPMI/PPMI/3505/MPRAGEadni/2010-12-23_10_50_52.0/S189286   \n",
       "\n",
       "  imputedSeq               sz                       px                  sp  \\\n",
       "0        MPR  (106, 256, 256)    16-bit signed integer     (1.4, 1.0, 1.0)   \n",
       "1        MPR  (256, 256, 192)  16-bit unsigned integer     (1.0, 1.0, 1.0)   \n",
       "2        MPR  (192, 192, 160)             32-bit float  (1.26, 1.25, 1.19)   \n",
       "3        MPR  (256, 256, 176)  16-bit unsigned integer     (1.0, 1.0, 1.0)   \n",
       "4        MPR  (220, 320, 208)  16-bit unsigned integer     (0.8, 0.8, 1.0)   \n",
       "5        MPR  (288, 288, 170)  16-bit unsigned integer   (0.92, 0.92, 1.2)   \n",
       "\n",
       "                             dir  \n",
       "0   (1, 0, 0, 0, -1, 0, 0, 0, 1)  \n",
       "1  (0, 0, -1, 1, 0, 0, 0, -1, 0)  \n",
       "2   (0, 0, 1, 0, 1, 0, -1, 0, 0)  \n",
       "3  (0, 0, -1, 1, 0, 0, 0, -1, 0)  \n",
       "4    (1, 0, 0, 0, 1, 0, 0, 0, 1)  \n",
       "5  (0, 0, -1, 1, 0, 0, 0, -1, 0)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR paths: \n",
      "ABIDE/ABIDE_1/50412/MP-RAGE/2000-01-01_00_00_00.0/S164292\n",
      "ABVIB/ABVIB/3830/MPRAGE/2012-06-18_11_46_49.0/S341930\n",
      "ADNI/ADNI1_Complete_1Yr_1.5T/ADNI/023_S_0139/MPR-R__GradWarp__B1_Correction__N3__Scaled/2007-02-09_09_44_27.0/S26343\n",
      "AIBL/AIBL/338/MPRAGE_SAG_ISO_p2_ND/2012-10-06_11_32_11.0/S231118\n",
      "ICMB/ICBM/UTHC_1098/MPRAGE_T1_AX_0.8_mm_TI-780/2009-03-13_13_01_09.0/S68959\n",
      "PPMI/PPMI/3505/MPRAGEadni/2010-12-23_10_50_52.0/S189286\n"
     ]
    }
   ],
   "source": [
    "# Data Source\n",
    "dset_src = '/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata'\n",
    "\n",
    "# Load example files\n",
    "inference_df = pd.read_csv(\"inference_example.csv\")\n",
    "display(inference_df)\n",
    "\n",
    "# We are doing inference on one example from each dataset\n",
    "mr_paths = inference_df.fn.values\n",
    "print(\"MR paths: \", *mr_paths, sep=\"\\n\")\n",
    "\n",
    "# for conditional segmentation models, we input also contains an atlas MR and segmentation\n",
    "example_atlas = {\n",
    "    'image': f'{dset_src}/samir_labels/50373-50453/50437/MP-RAGE/2000-01-01_00_00_00.0/S165191/ABIDE_50437_MRI_MP-RAGE_br_raw_20120830214425874_S165191_I329201_corrected_n4.nii',\n",
    "    'label': f'{dset_src}/samir_labels/50373-50453/50437/seg.nii'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb99b8",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11f65e",
   "metadata": {},
   "source": [
    "SITK: N4 bias correction and Re-orient to LAS coordinates. Note: N4 bias correction + re-orientation takes ~20s per input. In our workflow, we compute and save the result of this pre-processing step once over all the raw inputs.\n",
    "\n",
    "MONAI: Preprocess inputs to standard voxel spacing, intensity, center crop to standard dimensions. On-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "17fe0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dcm(fn):\n",
    "    \"\"\"IO for reading .dcm files in terminal folder\"\"\"\n",
    "    dcms = sitk.ImageSeriesReader_GetGDCMSeriesFileNames(fn)\n",
    "    if len(dcms) == 1: dcms = dcms[0]   \n",
    "    im = sitk.ReadImage(dcms, sitk.sitkFloat32)\n",
    "    return im\n",
    "\n",
    "def read_nii(fn):\n",
    "    \"\"\"IO for reading .nii files in terminal folder\"\"\"\n",
    "    if not fn.endswith(\".nii\"):\n",
    "        niis = [f for f in os.listdir(fn) if f.endswith(\".nii\") and not f.startswith(\"._\")]\n",
    "        nii   = niis[0]\n",
    "        im = sitk.ReadImage(f\"{fn}/{nii}\", sitk.sitkFloat32)    \n",
    "    else:\n",
    "        im = sitk.ReadImage(fn, sitk.sitkFloat32)\n",
    "    return im\n",
    "\n",
    "def n4_bias_correct(mr_path, do_write=False):\n",
    "    \"\"\" Perform N4 bias correction. Input: path to folder with .dcms or .niis. \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Read in image\n",
    "    try:\n",
    "        inputImage = read_nii(mr_path)\n",
    "    except:\n",
    "        inputImage = read_dcm(mr_path) \n",
    "\n",
    "    # Mask the head to estimate bias\n",
    "    maskImage = sitk.OtsuThreshold(inputImage, 0, 1, 200)\n",
    "\n",
    "    # Set corrector\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    corrector.SetMaximumNumberOfIterations([3] * 3)\n",
    "    corrected_image = corrector.Execute(inputImage, maskImage)\n",
    "\n",
    "    # LPI coordinates\n",
    "    corrected_image = sitk.DICOMOrient(inputImage, \"LAS\")\n",
    "    \n",
    "    # write image\n",
    "    if do_write:\n",
    "        corrected_fn = f\"{mr_path}/corrected_n4.nii\"\n",
    "        sitk.WriteImage(corrected_image, corrected_fn)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Elapsed: {elapsed:0.2f} s\")\n",
    "    \n",
    "    return corrected_image\n",
    "\n",
    "class UndoDict(ItemTransform):\n",
    "    \"\"\"Convert dictionary to tuple \"\"\"\n",
    "    split_idx = None\n",
    "\n",
    "    def __init__(self, keys=[\"image\"]):\n",
    "        self.keys = keys\n",
    "        \n",
    "    def encodes(self, d):\n",
    "        item = tuple(d[key] for key in self.keys)\n",
    "        # for condseg, 3-ch input\n",
    "        item = torch.cat(item, dim=0)\n",
    "        return item\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"UndoDict({self.keys})\"\n",
    "    \n",
    "def get_inference_transforms(keys, sp=(1.5,1.5,1.5), sz=(96,96,96), do_condseg = False):    \n",
    "    \"\"\"Preprocess inputs to standard voxel spacing, intensity, center crop to standard dimensions\"\"\"\n",
    "    \n",
    "    # Z-scale intensity values in image (not labels)\n",
    "    image_keys = [k for k in keys if \"image\" in k]\n",
    "    \n",
    "    # nearest neighbor interpolation for labels\n",
    "    interp_mode = tuple([\"bilinear\" if \"image\" in k else \"nearest\" for k in keys])\n",
    "        \n",
    "    return Compose([\n",
    "        Spacingd(keys, pixdim=sp, mode = interp_mode),\n",
    "        NormalizeIntensityd(image_keys, nonzero=True, channel_wise=False),\n",
    "        AddChanneld(keys),\n",
    "        SpatialPadd(keys, spatial_size=sz, method=\"symmetric\", mode=\"constant\"),\n",
    "        CenterSpatialCropd(keys, roi_size=sz),\n",
    "        ToTensord(keys),\n",
    "        UndoDict(keys),\n",
    "    ])\n",
    "\n",
    "# sitk obj and np array have different index conventions\n",
    "def sitk2np(obj): return np.swapaxes(sitk.GetArrayFromImage(obj), 0, 2)\n",
    "def np2sitk(arr): return sitk.GetImageFromArray(np.swapaxes(arr, 0, 2))\n",
    "\n",
    "def torch2sitk(t): return sitk.GetImageFromArray(torch.transpose(t, 0, 2))\n",
    "def sitk2torch(o): return torch.transpose(torch.tensor(sitk.GetArrayFromImage(o)), 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1efe13eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 25.69 s\n",
      "Elapsed: 24.29 s\n",
      "Elapsed: 9.00 s\n",
      "Elapsed: 36.08 s\n",
      "Elapsed: 34.02 s\n",
      "Elapsed: 28.59 s\n"
     ]
    }
   ],
   "source": [
    "# Optional: save the results as time-intensive, eg \n",
    "# our example atlas for condseg has already been N4 bias corrected and LAS oriented.\n",
    "n4_las_images = [n4_bias_correct(f\"{dset_src}/{mr_path}\") for mr_path in inference_df.fn.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0e85790e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET/VNET input shape:  torch.Size([3, 96, 96, 96]) . CONDSEG input shape:  torch.Size([3, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "# the atlas is only used for CONDSEG\n",
    "inputsd  = [\n",
    "    {\n",
    "        \"image\": sitk2np(im), \n",
    "        \"atlas_image\": sitk2np(read_nii(example_atlas[\"image\"])),\n",
    "        \"atlas_label\": sitk2np(read_nii(example_atlas[\"label\"]))\n",
    "    } \n",
    "    for im in n4_las_images\n",
    "]\n",
    "\n",
    "preproc_inputs         = get_inference_transforms(keys = (\"image\",))(inputsd)\n",
    "condseg_preproc_inputs = get_inference_transforms(keys = (\"image\",\"atlas_image\", \"atlas_label\"), do_condseg=True)(inputsd)\n",
    "\n",
    "print(\"UNET/VNET input shape: \", inputs[0].shape, \". CONDSEG input shape: \", condseg_inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d277967",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Post-process: (a)  keep largest connected component (b) majority vote among ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dddb36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source sitk 36_Microscopy_Colocalization_Distance_Analysis.html\n",
    "def get_largest_connected_component(binary_seg):\n",
    "    # connected components in sitkSeg\n",
    "    labeled_seg = sitk.ConnectedComponent(binary_seg)\n",
    "\n",
    "    # re-order labels according to size (at least 1_000 pixels = 10x10x10)\n",
    "    labeled_seg = sitk.RelabelComponent(labeled_seg, minimumObjectSize=1000, sortByObjectSize=True)\n",
    "\n",
    "    # return segm of largest label\n",
    "    binary_seg = labeled_seg == 1\n",
    "    \n",
    "    return binary_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39f94709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 96, 96, 96])\n",
      "torch.Size([6, 2, 96, 96, 96])\n",
      "6 (96, 96, 96)\n",
      "torch.Size([6, 1, 96, 96, 96])\n",
      "torch.Size([6, 2, 96, 96, 96])\n",
      "6 (96, 96, 96)\n",
      "torch.Size([6, 3, 96, 96, 96])\n",
      "torch.Size([6, 2, 96, 96, 96])\n",
      "6 (96, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "# each item in the list is the model's preds\n",
    "all_outputs = []\n",
    "all_preds = []\n",
    "\n",
    "for model_name, model_arch in ensemble_models.items():\n",
    "    \n",
    "    # get pre-processed inputs\n",
    "    inputs = preproc_inputs if model_name != \"CONDSEG_dice_loss\" else condseg_preproc_inputs\n",
    "    inputs = torch.stack(inputs, dim=0)\n",
    "    print(inputs.shape)\n",
    "    # apply model to the 6 example inputs\n",
    "    model_arch.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model_arch(inputs).cpu()\n",
    "        all_preds.append(preds)\n",
    "        print(preds.shape)\n",
    "        \n",
    "    # keep largest connected component for each model prediction\n",
    "    lcc = [get_largest_connected_component(torch2sitk(pred.argmax(0).byte())) for pred in preds]\n",
    "    all_outputs.append(lcc)\n",
    "    print(len(lcc), lcc[0].GetSize())\n",
    "    \n",
    "# get majority vote for each input\n",
    "def get_votes(i):\n",
    "    return [lcc[i] for lcc in all_outputs]\n",
    "\n",
    "labelForUndecidedPixels = 0\n",
    "majority_votes = [sitk.LabelVoting(get_votes(i), labelForUndecidedPixels) for i in range(len(preproc_inputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63406343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9626\n",
      "10126\n",
      "10222\n",
      "10661\n",
      "11766\n",
      "11556\n",
      "5531\n",
      "5578\n",
      "6341\n",
      "9529\n",
      "9528\n",
      "9507\n",
      "12876\n",
      "11665\n",
      "13773\n",
      "7834\n",
      "8561\n",
      "7219\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    for j in range(3):\n",
    "        print(len(torch.nonzero(sitk2torch(all_outputs[j][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "64c25367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get majority vote for each input\n",
    "def get_votes(i):\n",
    "    return [lcc[i] for lcc in outputs]\n",
    "\n",
    "labelForUndecidedPixels = 0\n",
    "majority_votes = [sitk.LabelVoting(get_votes(i), labelForUndecidedPixels) for i in range(len(preproc_inputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "49ea4a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(majority_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23893985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11304\n",
      "5771\n",
      "9532\n",
      "12689\n",
      "8024\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(len(torch.nonzero(sitk2torch(majority_votes[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85cff2",
   "metadata": {},
   "source": [
    "### Viz inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "94a7f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI bounding box\n",
    "\n",
    "def mask2bbox(mask):\n",
    "    k = torch.any(torch.any(mask, dim=0), dim=0) # 0 -> 1,2 -> 1 -> 2 left\n",
    "    j = torch.any(torch.any(mask, dim=0), dim=1) # 0 -> 1,2 -> 2 -> 1 left\n",
    "    i = torch.any(torch.any(mask, dim=1), dim=1) # 1 -> 0,2 -> 0 -> 0 left\n",
    "    \n",
    "    imin, imax = torch.where(i)[0][[0, -1]]\n",
    "    jmin, jmax = torch.where(j)[0][[0, -1]]\n",
    "    kmin, kmax = torch.where(k)[0][[0, -1]]\n",
    "    \n",
    "    # inclusive indices\n",
    "    return torch.tensor([imin, imax+1, jmin, jmax+1, kmin, kmax+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c3e185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import colors\n",
    "bin_cmap2  = colors.ListedColormap(['white', 'yellow'])\n",
    "\n",
    "def get_mid_idx(bbox, is_mask=False):\n",
    "    \n",
    "    if is_mask:\n",
    "        bbox = mask2bbox(bbox)\n",
    "       \n",
    "    axis_len = bbox[1] - bbox[0]\n",
    "    mid0     = bbox[0] + axis_len//2\n",
    "        \n",
    "    axis_len = bbox[3] - bbox[2]\n",
    "    mid1     = bbox[2] + axis_len//2\n",
    "\n",
    "    axis_len = bbox[5] - bbox[4]\n",
    "    mid2     = bbox[4] + axis_len//2\n",
    "\n",
    "    return mid0, mid1, mid2\n",
    "\n",
    "def imshows(ims):\n",
    "    nrow = len(ims)\n",
    "    ncol = 6\n",
    "    \n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(\n",
    "        ncol * 3, nrow * 3), facecolor='white')\n",
    "    for i, im_dict in enumerate(ims):\n",
    "        im = im_dict[\"image\"].detach().squeeze().cpu().numpy()\n",
    "        label = im_dict[\"label\"].detach().squeeze().cpu().numpy()\n",
    "        fname = im_dict[\"fname\"]\n",
    "        \n",
    "        # get bbox\n",
    "        bbox = mask2bbox(torch.tensor(label))\n",
    "        mids = get_mid_idx(bbox)\n",
    "        for axis_idx in range(3):\n",
    "            slice_idx = mids[axis_idx]\n",
    "\n",
    "            # plot image 0-2\n",
    "            ax  = axes[i, 0+axis_idx]\n",
    "            #ax.set_title(f\"{fname} Slice {slice_idx} (Axis {axis_idx})\")\n",
    "            ax.imshow(np.rot90(np.take(im, slice_idx, axis=axis_idx)), cmap=plt.cm.gray)\n",
    "            ax.imshow(np.rot90(np.take(label, slice_idx, axis=axis_idx)), alpha=0.2, cmap=bin_cmap2)\n",
    "            ax.axis(\"off\")\n",
    "                \n",
    "            # plot labels 3-5\n",
    "            ax        = axes[i, 3+axis_idx]\n",
    "            ax.axis(\"off\")\n",
    "            im_show = ax.imshow(np.rot90(np.take(label, slice_idx, axis=axis_idx)))\n",
    "            #fig.colorbar(im_show, ax=ax)\n",
    "            \n",
    "        # print fname\n",
    "        axes[i,0].set_title(f\"{fname}\")\n",
    "        axes[i,3].set_title(f\"Ensemble Seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8dab4c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-a863aea791fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msitk2torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajority_votes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask2bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mto_imshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fname\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdset_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-7fbb62b3cfb5>\u001b[0m in \u001b[0;36mmask2bbox\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 -> 0,2 -> 0 -> 0 left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mjmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "# test on one\n",
    "to_imshow = []\n",
    "\n",
    "for i in range(len(preproc_inputs)):\n",
    "    # get dset name\n",
    "    input_fn = mr_paths[i]\n",
    "    dset_name = input_fn[:input_fn.index(\"/\")]\n",
    "    \n",
    "    # get input and output\n",
    "    input_im   = preproc_inputs[i]\n",
    "    output_seg = sitk2torch(majority_votes[i])\n",
    "    \n",
    "    print(mask2bbox(output_seg))\n",
    "    \n",
    "    to_imshow.append({\"image\": input_im, \"label\": output_seg, \"fname\": dset_name})\n",
    "\n",
    "imshows(to_imshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339eeef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
